{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c0575b-4690-4b7d-b190-c143b4e74d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91942\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d2239a-faaa-4601-a288-cea1fe8c4990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the URL (use {page} for pagination):  https://www.amazon.in/\n",
      "Enter the type of data to scrape (headlines, product_details, job_listings):  watch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Failed to retrieve data. Exiting...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_data(url, data_type):\n",
    "    data = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        response = requests.get(url.format(page=page))\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to retrieve data. Exiting...\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if data_type == 'headlines':\n",
    "            headlines = soup.find_all('h2')  # Adjust the tag based on the website structure\n",
    "            for headline in headlines:\n",
    "                data.append({'headline': headline.get_text(strip=True)})\n",
    "\n",
    "        elif data_type == 'product_details':\n",
    "            products = soup.find_all('div', class_='product')  # Adjust the class based on the website structure\n",
    "            for product in products:\n",
    "                title = product.find('h3').get_text(strip=True)\n",
    "                price = product.find('span', class_='price').get_text(strip=True)\n",
    "                data.append({'title': title, 'price': price})\n",
    "\n",
    "        elif data_type == 'job_listings':\n",
    "            jobs = soup.find_all('div', class_='job')  # Adjust the class based on the website structure\n",
    "            for job in jobs:\n",
    "                title = job.find('h3').get_text(strip=True)\n",
    "                company = job.find('span', class_='company').get_text(strip=True)\n",
    "                location = job.find('span', class_='location').get_text(strip=True)\n",
    "                data.append({'title': title, 'company': company, 'location': location})\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid data type specified. Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Check for pagination\n",
    "        next_page = soup.find('a', class_='next')  # Adjust the class based on the website structure\n",
    "        if next_page:\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter the URL (use {page} for pagination): \")\n",
    "    data_type = input(\"Enter the type of data to scrape (headlines, product_details, job_listings): \")\n",
    "\n",
    "    scraped_data = scrape_data(url, data_type)\n",
    "    if scraped_data:\n",
    "        save_to_csv(scraped_data, 'scraped_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9886609b-788d-431d-afc0-cc04fa768784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the URL (use {page} for pagination):  https://remotive.io/remote-jobs?page=\n",
      "Enter the type of data to scrape (headlines, product_details, job_listings):  job_listings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://remotive.io/remote-jobs?page=...\n",
      "Failed to retrieve data from https://remotive.io/remote-jobs?page=. Status Code: 403\n",
      "No data found. Exiting...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_data(url, data_type):\n",
    "    data = []\n",
    "    page = 1\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        formatted_url = url.format(page=page)\n",
    "        print(f\"Scraping page {page}: {formatted_url}...\")\n",
    "        \n",
    "        response = requests.get(formatted_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data from {formatted_url}. Status Code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if data_type == 'headlines':\n",
    "            headlines = soup.find_all('h2')  # Adjust this based on the website\n",
    "            for headline in headlines:\n",
    "                data.append({'headline': headline.get_text(strip=True)})\n",
    "\n",
    "        elif data_type == 'product_details':\n",
    "            products = soup.find_all('div', class_='product')  # Adjust this\n",
    "            for product in products:\n",
    "                title = product.find('h3').get_text(strip=True) if product.find('h3') else \"N/A\"\n",
    "                price = product.find('span', class_='price').get_text(strip=True) if product.find('span', class_='price') else \"N/A\"\n",
    "                data.append({'title': title, 'price': price})\n",
    "\n",
    "        elif data_type == 'job_listings':\n",
    "            jobs = soup.find_all('div', class_='job')  # Adjust this\n",
    "            for job in jobs:\n",
    "                title = job.find('h3').get_text(strip=True) if job.find('h3') else \"N/A\"\n",
    "                company = job.find('span', class_='company').get_text(strip=True) if job.find('span', class_='company') else \"N/A\"\n",
    "                location = job.find('span', class_='location').get_text(strip=True) if job.find('span', class_='location') else \"N/A\"\n",
    "                data.append({'title': title, 'company': company, 'location': location})\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid data type specified. Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Handle pagination\n",
    "        next_page = soup.find('a', class_='next')  # Adjust this\n",
    "        if next_page:\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    if not data:\n",
    "        print(\"No data found. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter the URL (use {page} for pagination): \")\n",
    "    data_type = input(\"Enter the type of data to scrape (headlines, product_details, job_listings): \")\n",
    "\n",
    "    scraped_data = scrape_data(url, data_type)\n",
    "    save_to_csv(scraped_data, 'scraped_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394cdd0-e779-4291-b74d-048e274b7672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
